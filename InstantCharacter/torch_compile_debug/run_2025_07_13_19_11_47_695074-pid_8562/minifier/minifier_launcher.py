
import os
os.environ['TORCHDYNAMO_REPRO_AFTER'] = 'dynamo'
os.environ['PYTORCH_VERSION'] = '2.7.1'
os.environ['TORCHINDUCTOR_CACHE_DIR'] = '/tmp/torchinductor_nikbauer34'
os.environ['TRITON_CACHE_DIR'] = '/tmp/torchinductor_nikbauer34/triton/0'

from math import inf
import torch
from torch import tensor, device
import torch.fx as fx
import torch._dynamo
from torch._dynamo.testing import rand_strided
from torch._dynamo.debug_utils import run_fwd_maybe_bwd

import torch._dynamo.config
import torch._inductor.config
import torch._functorch.config
import torch.fx.experimental._config
torch._dynamo.config.recompile_limit = 1024
torch._dynamo.config.assume_static_by_default = False
torch._dynamo.config.capture_scalar_outputs = True
torch._dynamo.config.capture_dynamic_output_shape_ops = True
torch._inductor.config.max_autotune = False
torch._inductor.config.coordinate_descent_tuning = False
torch._inductor.config.benchmark_kernel = False
torch._inductor.config.triton.cudagraphs = False
torch._inductor.config.triton.store_cubin = False
torch._inductor.config.triton.codegen_upcast_to_fp32 = True
torch._inductor.config.test_configs.runtime_triton_dtype_assert = False
torch._functorch.config.functionalize_rng_ops = False
torch._functorch.config.fake_tensor_allow_unsafe_data_ptr_access = True
torch._functorch.config.unlift_effect_tokens = False






from torch.nn import *
class Repro(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()



    def forward(self, L_args_0_ : torch.Tensor, L_self_modules_text_model_modules_embeddings_modules_position_embedding_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_embeddings_buffers_position_ids_ : torch.Tensor, L_self_modules_text_model_modules_embeddings_modules_token_embedding_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_bias_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_final_layer_norm_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_text_model_modules_final_layer_norm_parameters_bias_ : torch.nn.parameter.Parameter):
        l_args_0_ = L_args_0_
        l_self_modules_text_model_modules_embeddings_modules_position_embedding_parameters_weight_ = L_self_modules_text_model_modules_embeddings_modules_position_embedding_parameters_weight_
        l_self_modules_text_model_modules_embeddings_buffers_position_ids_ = L_self_modules_text_model_modules_embeddings_buffers_position_ids_
        l_self_modules_text_model_modules_embeddings_modules_token_embedding_parameters_weight_ = L_self_modules_text_model_modules_embeddings_modules_token_embedding_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_bias_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_weight_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_weight_
        l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_bias_ = L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_bias_
        l_self_modules_text_model_modules_final_layer_norm_parameters_weight_ = L_self_modules_text_model_modules_final_layer_norm_parameters_weight_
        l_self_modules_text_model_modules_final_layer_norm_parameters_bias_ = L_self_modules_text_model_modules_final_layer_norm_parameters_bias_
        size = l_args_0_.size()
        getitem_1 = size[1];  size = None
        input_ids = l_args_0_.view(-1, getitem_1);  l_args_0_ = None
        size_1 = input_ids.size()
        getitem_3 = size_1[1];  size_1 = None
        position_ids = l_self_modules_text_model_modules_embeddings_buffers_position_ids_[(slice(None, None, None), slice(None, getitem_3, None))];  l_self_modules_text_model_modules_embeddings_buffers_position_ids_ = getitem_3 = None
        inputs_embeds = torch.nn.functional.embedding(input_ids, l_self_modules_text_model_modules_embeddings_modules_token_embedding_parameters_weight_, None, None, 2.0, False, False);  l_self_modules_text_model_modules_embeddings_modules_token_embedding_parameters_weight_ = None
        position_embeddings = torch.nn.functional.embedding(position_ids, l_self_modules_text_model_modules_embeddings_modules_position_embedding_parameters_weight_, None, None, 2.0, False, False);  position_ids = l_self_modules_text_model_modules_embeddings_modules_position_embedding_parameters_weight_ = None
        embeddings = inputs_embeds + position_embeddings;  inputs_embeds = position_embeddings = None
        add_1 = 0 + getitem_1
        sub = add_1 - getitem_1;  add_1 = None
        mask = torch.full((getitem_1, getitem_1), -3.3895313892515355e+38, device = device(type='cuda', index=0))
        mask_cond = torch.arange(77, device = device(type='cuda', index=0))
        add_2 = mask_cond + 1
        view_1 = add_2.view(77, 1);  add_2 = None
        lt = mask_cond < view_1;  mask_cond = view_1 = None
        masked_fill_ = mask.masked_fill_(lt, 0);  lt = masked_fill_ = None
        mask_1 = mask.to(torch.bfloat16);  mask = None
        getitem_5 = mask_1[(None, None, slice(None, None, None), slice(None, None, None))];  mask_1 = None
        add_3 = getitem_1 + sub;  sub = None
        causal_4d_mask = getitem_5.expand(1, 1, getitem_1, add_3);  getitem_5 = getitem_1 = add_3 = None
        hidden_states = torch.nn.functional.layer_norm(embeddings, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_bias_ = None
        queries = torch._C._nn.linear(hidden_states, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys = torch._C._nn.linear(hidden_states, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values = torch._C._nn.linear(hidden_states, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_2 = queries.view(1, 77, -1, 64);  queries = None
        queries_1 = view_2.transpose(1, 2);  view_2 = None
        view_3 = keys.view(1, 77, -1, 64);  keys = None
        keys_1 = view_3.transpose(1, 2);  view_3 = None
        view_4 = values.view(1, 77, -1, 64);  values = None
        values_1 = view_4.transpose(1, 2);  view_4 = None
        attention_mask = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query = queries_1.contiguous();  queries_1 = None
        key = keys_1.contiguous();  keys_1 = None
        value = values_1.contiguous();  values_1 = None
        attn_output = torch._C._nn.scaled_dot_product_attention(query, key, value, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.125, is_causal = False);  query = key = value = attention_mask = None
        transpose_3 = attn_output.transpose(1, 2);  attn_output = None
        attn_output_1 = transpose_3.contiguous();  transpose_3 = None
        reshape = attn_output_1.reshape(1, 77, 768);  attn_output_1 = None
        attn_output_2 = reshape.contiguous();  reshape = None
        attn_output_3 = torch._C._nn.linear(attn_output_2, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_2 = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_1 = embeddings + attn_output_3;  embeddings = attn_output_3 = None
        hidden_states_2 = torch.nn.functional.layer_norm(hidden_states_1, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_bias_ = None
        hidden_states_3 = torch._C._nn.linear(hidden_states_2, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_2 = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_bias_ = None
        mul = 1.702 * hidden_states_3
        sigmoid = torch.sigmoid(mul);  mul = None
        hidden_states_4 = hidden_states_3 * sigmoid;  hidden_states_3 = sigmoid = None
        hidden_states_5 = torch._C._nn.linear(hidden_states_4, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_4 = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_6 = hidden_states_1 + hidden_states_5;  hidden_states_1 = hidden_states_5 = None
        hidden_states_7 = torch.nn.functional.layer_norm(hidden_states_6, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_bias_ = None
        queries_2 = torch._C._nn.linear(hidden_states_7, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_2 = torch._C._nn.linear(hidden_states_7, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_2 = torch._C._nn.linear(hidden_states_7, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_7 = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_5 = queries_2.view(1, 77, -1, 64);  queries_2 = None
        queries_3 = view_5.transpose(1, 2);  view_5 = None
        view_6 = keys_2.view(1, 77, -1, 64);  keys_2 = None
        keys_3 = view_6.transpose(1, 2);  view_6 = None
        view_7 = values_2.view(1, 77, -1, 64);  values_2 = None
        values_3 = view_7.transpose(1, 2);  view_7 = None
        attention_mask_1 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_1 = queries_3.contiguous();  queries_3 = None
        key_1 = keys_3.contiguous();  keys_3 = None
        value_1 = values_3.contiguous();  values_3 = None
        attn_output_4 = torch._C._nn.scaled_dot_product_attention(query_1, key_1, value_1, attn_mask = attention_mask_1, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_1 = key_1 = value_1 = attention_mask_1 = None
        transpose_7 = attn_output_4.transpose(1, 2);  attn_output_4 = None
        attn_output_5 = transpose_7.contiguous();  transpose_7 = None
        reshape_1 = attn_output_5.reshape(1, 77, 768);  attn_output_5 = None
        attn_output_6 = reshape_1.contiguous();  reshape_1 = None
        attn_output_7 = torch._C._nn.linear(attn_output_6, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_6 = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_8 = hidden_states_6 + attn_output_7;  hidden_states_6 = attn_output_7 = None
        hidden_states_9 = torch.nn.functional.layer_norm(hidden_states_8, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_bias_ = None
        hidden_states_10 = torch._C._nn.linear(hidden_states_9, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_9 = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_2 = 1.702 * hidden_states_10
        sigmoid_1 = torch.sigmoid(mul_2);  mul_2 = None
        hidden_states_11 = hidden_states_10 * sigmoid_1;  hidden_states_10 = sigmoid_1 = None
        hidden_states_12 = torch._C._nn.linear(hidden_states_11, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_11 = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_13 = hidden_states_8 + hidden_states_12;  hidden_states_8 = hidden_states_12 = None
        hidden_states_14 = torch.nn.functional.layer_norm(hidden_states_13, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_bias_ = None
        queries_4 = torch._C._nn.linear(hidden_states_14, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_4 = torch._C._nn.linear(hidden_states_14, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_4 = torch._C._nn.linear(hidden_states_14, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_14 = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_8 = queries_4.view(1, 77, -1, 64);  queries_4 = None
        queries_5 = view_8.transpose(1, 2);  view_8 = None
        view_9 = keys_4.view(1, 77, -1, 64);  keys_4 = None
        keys_5 = view_9.transpose(1, 2);  view_9 = None
        view_10 = values_4.view(1, 77, -1, 64);  values_4 = None
        values_5 = view_10.transpose(1, 2);  view_10 = None
        attention_mask_2 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_2 = queries_5.contiguous();  queries_5 = None
        key_2 = keys_5.contiguous();  keys_5 = None
        value_2 = values_5.contiguous();  values_5 = None
        attn_output_8 = torch._C._nn.scaled_dot_product_attention(query_2, key_2, value_2, attn_mask = attention_mask_2, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_2 = key_2 = value_2 = attention_mask_2 = None
        transpose_11 = attn_output_8.transpose(1, 2);  attn_output_8 = None
        attn_output_9 = transpose_11.contiguous();  transpose_11 = None
        reshape_2 = attn_output_9.reshape(1, 77, 768);  attn_output_9 = None
        attn_output_10 = reshape_2.contiguous();  reshape_2 = None
        attn_output_11 = torch._C._nn.linear(attn_output_10, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_10 = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_15 = hidden_states_13 + attn_output_11;  hidden_states_13 = attn_output_11 = None
        hidden_states_16 = torch.nn.functional.layer_norm(hidden_states_15, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_bias_ = None
        hidden_states_17 = torch._C._nn.linear(hidden_states_16, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_16 = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_4 = 1.702 * hidden_states_17
        sigmoid_2 = torch.sigmoid(mul_4);  mul_4 = None
        hidden_states_18 = hidden_states_17 * sigmoid_2;  hidden_states_17 = sigmoid_2 = None
        hidden_states_19 = torch._C._nn.linear(hidden_states_18, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_18 = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_20 = hidden_states_15 + hidden_states_19;  hidden_states_15 = hidden_states_19 = None
        hidden_states_21 = torch.nn.functional.layer_norm(hidden_states_20, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_bias_ = None
        queries_6 = torch._C._nn.linear(hidden_states_21, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_6 = torch._C._nn.linear(hidden_states_21, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_6 = torch._C._nn.linear(hidden_states_21, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_21 = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_11 = queries_6.view(1, 77, -1, 64);  queries_6 = None
        queries_7 = view_11.transpose(1, 2);  view_11 = None
        view_12 = keys_6.view(1, 77, -1, 64);  keys_6 = None
        keys_7 = view_12.transpose(1, 2);  view_12 = None
        view_13 = values_6.view(1, 77, -1, 64);  values_6 = None
        values_7 = view_13.transpose(1, 2);  view_13 = None
        attention_mask_3 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_3 = queries_7.contiguous();  queries_7 = None
        key_3 = keys_7.contiguous();  keys_7 = None
        value_3 = values_7.contiguous();  values_7 = None
        attn_output_12 = torch._C._nn.scaled_dot_product_attention(query_3, key_3, value_3, attn_mask = attention_mask_3, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_3 = key_3 = value_3 = attention_mask_3 = None
        transpose_15 = attn_output_12.transpose(1, 2);  attn_output_12 = None
        attn_output_13 = transpose_15.contiguous();  transpose_15 = None
        reshape_3 = attn_output_13.reshape(1, 77, 768);  attn_output_13 = None
        attn_output_14 = reshape_3.contiguous();  reshape_3 = None
        attn_output_15 = torch._C._nn.linear(attn_output_14, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_14 = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_22 = hidden_states_20 + attn_output_15;  hidden_states_20 = attn_output_15 = None
        hidden_states_23 = torch.nn.functional.layer_norm(hidden_states_22, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_bias_ = None
        hidden_states_24 = torch._C._nn.linear(hidden_states_23, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_23 = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_6 = 1.702 * hidden_states_24
        sigmoid_3 = torch.sigmoid(mul_6);  mul_6 = None
        hidden_states_25 = hidden_states_24 * sigmoid_3;  hidden_states_24 = sigmoid_3 = None
        hidden_states_26 = torch._C._nn.linear(hidden_states_25, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_25 = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_27 = hidden_states_22 + hidden_states_26;  hidden_states_22 = hidden_states_26 = None
        hidden_states_28 = torch.nn.functional.layer_norm(hidden_states_27, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_bias_ = None
        queries_8 = torch._C._nn.linear(hidden_states_28, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_8 = torch._C._nn.linear(hidden_states_28, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_8 = torch._C._nn.linear(hidden_states_28, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_28 = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_14 = queries_8.view(1, 77, -1, 64);  queries_8 = None
        queries_9 = view_14.transpose(1, 2);  view_14 = None
        view_15 = keys_8.view(1, 77, -1, 64);  keys_8 = None
        keys_9 = view_15.transpose(1, 2);  view_15 = None
        view_16 = values_8.view(1, 77, -1, 64);  values_8 = None
        values_9 = view_16.transpose(1, 2);  view_16 = None
        attention_mask_4 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_4 = queries_9.contiguous();  queries_9 = None
        key_4 = keys_9.contiguous();  keys_9 = None
        value_4 = values_9.contiguous();  values_9 = None
        attn_output_16 = torch._C._nn.scaled_dot_product_attention(query_4, key_4, value_4, attn_mask = attention_mask_4, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_4 = key_4 = value_4 = attention_mask_4 = None
        transpose_19 = attn_output_16.transpose(1, 2);  attn_output_16 = None
        attn_output_17 = transpose_19.contiguous();  transpose_19 = None
        reshape_4 = attn_output_17.reshape(1, 77, 768);  attn_output_17 = None
        attn_output_18 = reshape_4.contiguous();  reshape_4 = None
        attn_output_19 = torch._C._nn.linear(attn_output_18, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_18 = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_29 = hidden_states_27 + attn_output_19;  hidden_states_27 = attn_output_19 = None
        hidden_states_30 = torch.nn.functional.layer_norm(hidden_states_29, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_bias_ = None
        hidden_states_31 = torch._C._nn.linear(hidden_states_30, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_30 = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_8 = 1.702 * hidden_states_31
        sigmoid_4 = torch.sigmoid(mul_8);  mul_8 = None
        hidden_states_32 = hidden_states_31 * sigmoid_4;  hidden_states_31 = sigmoid_4 = None
        hidden_states_33 = torch._C._nn.linear(hidden_states_32, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_32 = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_34 = hidden_states_29 + hidden_states_33;  hidden_states_29 = hidden_states_33 = None
        hidden_states_35 = torch.nn.functional.layer_norm(hidden_states_34, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_bias_ = None
        queries_10 = torch._C._nn.linear(hidden_states_35, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_10 = torch._C._nn.linear(hidden_states_35, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_10 = torch._C._nn.linear(hidden_states_35, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_35 = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_17 = queries_10.view(1, 77, -1, 64);  queries_10 = None
        queries_11 = view_17.transpose(1, 2);  view_17 = None
        view_18 = keys_10.view(1, 77, -1, 64);  keys_10 = None
        keys_11 = view_18.transpose(1, 2);  view_18 = None
        view_19 = values_10.view(1, 77, -1, 64);  values_10 = None
        values_11 = view_19.transpose(1, 2);  view_19 = None
        attention_mask_5 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_5 = queries_11.contiguous();  queries_11 = None
        key_5 = keys_11.contiguous();  keys_11 = None
        value_5 = values_11.contiguous();  values_11 = None
        attn_output_20 = torch._C._nn.scaled_dot_product_attention(query_5, key_5, value_5, attn_mask = attention_mask_5, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_5 = key_5 = value_5 = attention_mask_5 = None
        transpose_23 = attn_output_20.transpose(1, 2);  attn_output_20 = None
        attn_output_21 = transpose_23.contiguous();  transpose_23 = None
        reshape_5 = attn_output_21.reshape(1, 77, 768);  attn_output_21 = None
        attn_output_22 = reshape_5.contiguous();  reshape_5 = None
        attn_output_23 = torch._C._nn.linear(attn_output_22, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_22 = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_36 = hidden_states_34 + attn_output_23;  hidden_states_34 = attn_output_23 = None
        hidden_states_37 = torch.nn.functional.layer_norm(hidden_states_36, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_bias_ = None
        hidden_states_38 = torch._C._nn.linear(hidden_states_37, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_37 = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_10 = 1.702 * hidden_states_38
        sigmoid_5 = torch.sigmoid(mul_10);  mul_10 = None
        hidden_states_39 = hidden_states_38 * sigmoid_5;  hidden_states_38 = sigmoid_5 = None
        hidden_states_40 = torch._C._nn.linear(hidden_states_39, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_39 = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_41 = hidden_states_36 + hidden_states_40;  hidden_states_36 = hidden_states_40 = None
        hidden_states_42 = torch.nn.functional.layer_norm(hidden_states_41, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_bias_ = None
        queries_12 = torch._C._nn.linear(hidden_states_42, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_12 = torch._C._nn.linear(hidden_states_42, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_12 = torch._C._nn.linear(hidden_states_42, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_42 = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_20 = queries_12.view(1, 77, -1, 64);  queries_12 = None
        queries_13 = view_20.transpose(1, 2);  view_20 = None
        view_21 = keys_12.view(1, 77, -1, 64);  keys_12 = None
        keys_13 = view_21.transpose(1, 2);  view_21 = None
        view_22 = values_12.view(1, 77, -1, 64);  values_12 = None
        values_13 = view_22.transpose(1, 2);  view_22 = None
        attention_mask_6 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_6 = queries_13.contiguous();  queries_13 = None
        key_6 = keys_13.contiguous();  keys_13 = None
        value_6 = values_13.contiguous();  values_13 = None
        attn_output_24 = torch._C._nn.scaled_dot_product_attention(query_6, key_6, value_6, attn_mask = attention_mask_6, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_6 = key_6 = value_6 = attention_mask_6 = None
        transpose_27 = attn_output_24.transpose(1, 2);  attn_output_24 = None
        attn_output_25 = transpose_27.contiguous();  transpose_27 = None
        reshape_6 = attn_output_25.reshape(1, 77, 768);  attn_output_25 = None
        attn_output_26 = reshape_6.contiguous();  reshape_6 = None
        attn_output_27 = torch._C._nn.linear(attn_output_26, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_26 = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_43 = hidden_states_41 + attn_output_27;  hidden_states_41 = attn_output_27 = None
        hidden_states_44 = torch.nn.functional.layer_norm(hidden_states_43, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_bias_ = None
        hidden_states_45 = torch._C._nn.linear(hidden_states_44, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_44 = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_12 = 1.702 * hidden_states_45
        sigmoid_6 = torch.sigmoid(mul_12);  mul_12 = None
        hidden_states_46 = hidden_states_45 * sigmoid_6;  hidden_states_45 = sigmoid_6 = None
        hidden_states_47 = torch._C._nn.linear(hidden_states_46, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_46 = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_48 = hidden_states_43 + hidden_states_47;  hidden_states_43 = hidden_states_47 = None
        hidden_states_49 = torch.nn.functional.layer_norm(hidden_states_48, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_bias_ = None
        queries_14 = torch._C._nn.linear(hidden_states_49, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_14 = torch._C._nn.linear(hidden_states_49, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_14 = torch._C._nn.linear(hidden_states_49, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_49 = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_23 = queries_14.view(1, 77, -1, 64);  queries_14 = None
        queries_15 = view_23.transpose(1, 2);  view_23 = None
        view_24 = keys_14.view(1, 77, -1, 64);  keys_14 = None
        keys_15 = view_24.transpose(1, 2);  view_24 = None
        view_25 = values_14.view(1, 77, -1, 64);  values_14 = None
        values_15 = view_25.transpose(1, 2);  view_25 = None
        attention_mask_7 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_7 = queries_15.contiguous();  queries_15 = None
        key_7 = keys_15.contiguous();  keys_15 = None
        value_7 = values_15.contiguous();  values_15 = None
        attn_output_28 = torch._C._nn.scaled_dot_product_attention(query_7, key_7, value_7, attn_mask = attention_mask_7, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_7 = key_7 = value_7 = attention_mask_7 = None
        transpose_31 = attn_output_28.transpose(1, 2);  attn_output_28 = None
        attn_output_29 = transpose_31.contiguous();  transpose_31 = None
        reshape_7 = attn_output_29.reshape(1, 77, 768);  attn_output_29 = None
        attn_output_30 = reshape_7.contiguous();  reshape_7 = None
        attn_output_31 = torch._C._nn.linear(attn_output_30, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_30 = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_50 = hidden_states_48 + attn_output_31;  hidden_states_48 = attn_output_31 = None
        hidden_states_51 = torch.nn.functional.layer_norm(hidden_states_50, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_bias_ = None
        hidden_states_52 = torch._C._nn.linear(hidden_states_51, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_51 = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_14 = 1.702 * hidden_states_52
        sigmoid_7 = torch.sigmoid(mul_14);  mul_14 = None
        hidden_states_53 = hidden_states_52 * sigmoid_7;  hidden_states_52 = sigmoid_7 = None
        hidden_states_54 = torch._C._nn.linear(hidden_states_53, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_53 = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_55 = hidden_states_50 + hidden_states_54;  hidden_states_50 = hidden_states_54 = None
        hidden_states_56 = torch.nn.functional.layer_norm(hidden_states_55, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_bias_ = None
        queries_16 = torch._C._nn.linear(hidden_states_56, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_16 = torch._C._nn.linear(hidden_states_56, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_16 = torch._C._nn.linear(hidden_states_56, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_56 = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_26 = queries_16.view(1, 77, -1, 64);  queries_16 = None
        queries_17 = view_26.transpose(1, 2);  view_26 = None
        view_27 = keys_16.view(1, 77, -1, 64);  keys_16 = None
        keys_17 = view_27.transpose(1, 2);  view_27 = None
        view_28 = values_16.view(1, 77, -1, 64);  values_16 = None
        values_17 = view_28.transpose(1, 2);  view_28 = None
        attention_mask_8 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_8 = queries_17.contiguous();  queries_17 = None
        key_8 = keys_17.contiguous();  keys_17 = None
        value_8 = values_17.contiguous();  values_17 = None
        attn_output_32 = torch._C._nn.scaled_dot_product_attention(query_8, key_8, value_8, attn_mask = attention_mask_8, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_8 = key_8 = value_8 = attention_mask_8 = None
        transpose_35 = attn_output_32.transpose(1, 2);  attn_output_32 = None
        attn_output_33 = transpose_35.contiguous();  transpose_35 = None
        reshape_8 = attn_output_33.reshape(1, 77, 768);  attn_output_33 = None
        attn_output_34 = reshape_8.contiguous();  reshape_8 = None
        attn_output_35 = torch._C._nn.linear(attn_output_34, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_34 = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_57 = hidden_states_55 + attn_output_35;  hidden_states_55 = attn_output_35 = None
        hidden_states_58 = torch.nn.functional.layer_norm(hidden_states_57, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_bias_ = None
        hidden_states_59 = torch._C._nn.linear(hidden_states_58, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_58 = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_16 = 1.702 * hidden_states_59
        sigmoid_8 = torch.sigmoid(mul_16);  mul_16 = None
        hidden_states_60 = hidden_states_59 * sigmoid_8;  hidden_states_59 = sigmoid_8 = None
        hidden_states_61 = torch._C._nn.linear(hidden_states_60, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_60 = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_62 = hidden_states_57 + hidden_states_61;  hidden_states_57 = hidden_states_61 = None
        hidden_states_63 = torch.nn.functional.layer_norm(hidden_states_62, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_bias_ = None
        queries_18 = torch._C._nn.linear(hidden_states_63, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_18 = torch._C._nn.linear(hidden_states_63, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_18 = torch._C._nn.linear(hidden_states_63, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_63 = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_29 = queries_18.view(1, 77, -1, 64);  queries_18 = None
        queries_19 = view_29.transpose(1, 2);  view_29 = None
        view_30 = keys_18.view(1, 77, -1, 64);  keys_18 = None
        keys_19 = view_30.transpose(1, 2);  view_30 = None
        view_31 = values_18.view(1, 77, -1, 64);  values_18 = None
        values_19 = view_31.transpose(1, 2);  view_31 = None
        attention_mask_9 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_9 = queries_19.contiguous();  queries_19 = None
        key_9 = keys_19.contiguous();  keys_19 = None
        value_9 = values_19.contiguous();  values_19 = None
        attn_output_36 = torch._C._nn.scaled_dot_product_attention(query_9, key_9, value_9, attn_mask = attention_mask_9, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_9 = key_9 = value_9 = attention_mask_9 = None
        transpose_39 = attn_output_36.transpose(1, 2);  attn_output_36 = None
        attn_output_37 = transpose_39.contiguous();  transpose_39 = None
        reshape_9 = attn_output_37.reshape(1, 77, 768);  attn_output_37 = None
        attn_output_38 = reshape_9.contiguous();  reshape_9 = None
        attn_output_39 = torch._C._nn.linear(attn_output_38, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_38 = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_64 = hidden_states_62 + attn_output_39;  hidden_states_62 = attn_output_39 = None
        hidden_states_65 = torch.nn.functional.layer_norm(hidden_states_64, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_bias_ = None
        hidden_states_66 = torch._C._nn.linear(hidden_states_65, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_65 = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_18 = 1.702 * hidden_states_66
        sigmoid_9 = torch.sigmoid(mul_18);  mul_18 = None
        hidden_states_67 = hidden_states_66 * sigmoid_9;  hidden_states_66 = sigmoid_9 = None
        hidden_states_68 = torch._C._nn.linear(hidden_states_67, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_67 = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_69 = hidden_states_64 + hidden_states_68;  hidden_states_64 = hidden_states_68 = None
        hidden_states_70 = torch.nn.functional.layer_norm(hidden_states_69, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_bias_ = None
        queries_20 = torch._C._nn.linear(hidden_states_70, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_20 = torch._C._nn.linear(hidden_states_70, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_20 = torch._C._nn.linear(hidden_states_70, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_70 = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_32 = queries_20.view(1, 77, -1, 64);  queries_20 = None
        queries_21 = view_32.transpose(1, 2);  view_32 = None
        view_33 = keys_20.view(1, 77, -1, 64);  keys_20 = None
        keys_21 = view_33.transpose(1, 2);  view_33 = None
        view_34 = values_20.view(1, 77, -1, 64);  values_20 = None
        values_21 = view_34.transpose(1, 2);  view_34 = None
        attention_mask_10 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))]
        query_10 = queries_21.contiguous();  queries_21 = None
        key_10 = keys_21.contiguous();  keys_21 = None
        value_10 = values_21.contiguous();  values_21 = None
        attn_output_40 = torch._C._nn.scaled_dot_product_attention(query_10, key_10, value_10, attn_mask = attention_mask_10, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_10 = key_10 = value_10 = attention_mask_10 = None
        transpose_43 = attn_output_40.transpose(1, 2);  attn_output_40 = None
        attn_output_41 = transpose_43.contiguous();  transpose_43 = None
        reshape_10 = attn_output_41.reshape(1, 77, 768);  attn_output_41 = None
        attn_output_42 = reshape_10.contiguous();  reshape_10 = None
        attn_output_43 = torch._C._nn.linear(attn_output_42, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_42 = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_71 = hidden_states_69 + attn_output_43;  hidden_states_69 = attn_output_43 = None
        hidden_states_72 = torch.nn.functional.layer_norm(hidden_states_71, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_bias_ = None
        hidden_states_73 = torch._C._nn.linear(hidden_states_72, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_72 = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_20 = 1.702 * hidden_states_73
        sigmoid_10 = torch.sigmoid(mul_20);  mul_20 = None
        hidden_states_74 = hidden_states_73 * sigmoid_10;  hidden_states_73 = sigmoid_10 = None
        hidden_states_75 = torch._C._nn.linear(hidden_states_74, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_74 = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_76 = hidden_states_71 + hidden_states_75;  hidden_states_71 = hidden_states_75 = None
        hidden_states_77 = torch.nn.functional.layer_norm(hidden_states_76, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_bias_ = None
        queries_22 = torch._C._nn.linear(hidden_states_77, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_bias_ = None
        keys_22 = torch._C._nn.linear(hidden_states_77, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_bias_);  l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_bias_ = None
        values_22 = torch._C._nn.linear(hidden_states_77, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_bias_);  hidden_states_77 = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_bias_ = None
        view_35 = queries_22.view(1, 77, -1, 64);  queries_22 = None
        queries_23 = view_35.transpose(1, 2);  view_35 = None
        view_36 = keys_22.view(1, 77, -1, 64);  keys_22 = None
        keys_23 = view_36.transpose(1, 2);  view_36 = None
        view_37 = values_22.view(1, 77, -1, 64);  values_22 = None
        values_23 = view_37.transpose(1, 2);  view_37 = None
        attention_mask_11 = causal_4d_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 77, None))];  causal_4d_mask = None
        query_11 = queries_23.contiguous();  queries_23 = None
        key_11 = keys_23.contiguous();  keys_23 = None
        value_11 = values_23.contiguous();  values_23 = None
        attn_output_44 = torch._C._nn.scaled_dot_product_attention(query_11, key_11, value_11, attn_mask = attention_mask_11, dropout_p = 0.0, scale = 0.125, is_causal = False);  query_11 = key_11 = value_11 = attention_mask_11 = None
        transpose_47 = attn_output_44.transpose(1, 2);  attn_output_44 = None
        attn_output_45 = transpose_47.contiguous();  transpose_47 = None
        reshape_11 = attn_output_45.reshape(1, 77, 768);  attn_output_45 = None
        attn_output_46 = reshape_11.contiguous();  reshape_11 = None
        attn_output_47 = torch._C._nn.linear(attn_output_46, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_bias_);  attn_output_46 = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_bias_ = None
        hidden_states_78 = hidden_states_76 + attn_output_47;  hidden_states_76 = attn_output_47 = None
        hidden_states_79 = torch.nn.functional.layer_norm(hidden_states_78, (768,), l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_bias_, 1e-05);  l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_bias_ = None
        hidden_states_80 = torch._C._nn.linear(hidden_states_79, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_bias_);  hidden_states_79 = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_bias_ = None
        mul_22 = 1.702 * hidden_states_80
        sigmoid_11 = torch.sigmoid(mul_22);  mul_22 = None
        hidden_states_81 = hidden_states_80 * sigmoid_11;  hidden_states_80 = sigmoid_11 = None
        hidden_states_82 = torch._C._nn.linear(hidden_states_81, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_weight_, l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_bias_);  hidden_states_81 = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_weight_ = l_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_bias_ = None
        hidden_states_83 = hidden_states_78 + hidden_states_82;  hidden_states_78 = hidden_states_82 = None
        last_hidden_state = torch.nn.functional.layer_norm(hidden_states_83, (768,), l_self_modules_text_model_modules_final_layer_norm_parameters_weight_, l_self_modules_text_model_modules_final_layer_norm_parameters_bias_, 1e-05);  hidden_states_83 = l_self_modules_text_model_modules_final_layer_norm_parameters_weight_ = l_self_modules_text_model_modules_final_layer_norm_parameters_bias_ = None
        arange_1 = torch.arange(1, device = device(type='cuda', index=0))
        to_1 = input_ids.to(dtype = torch.int32, device = device(type='cuda', index=0));  input_ids = None
        argmax = to_1.argmax(dim = -1);  to_1 = None
        pooled_output = last_hidden_state[(arange_1, argmax)];  arange_1 = argmax = None
        return (last_hidden_state, pooled_output)


mod = Repro()

def load_args(reader):
    buf0 = reader.storage('6076830ea700eb70f444e25cdc7b9b4780620dc2', 616, device=device(type='cuda', index=0), dtype_hint=torch.int64)
    reader.tensor(buf0, (1, 77), dtype=torch.int64, is_leaf=True)  # L_args_0_
    buf1 = reader.storage('cc283d3b117b6327fc7bf0c5946cd2a1815cddcc', 118272, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf1, (77, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_embeddings_modules_position_embedding_parameters_weight_
    buf2 = reader.storage('e7e7b2e65acc3f7fa8afc5c62d8f7c0d988bffbc', 616, device=device(type='cuda', index=0), dtype_hint=torch.int64)
    reader.tensor(buf2, (1, 77), dtype=torch.int64, is_leaf=True)  # L_self_modules_text_model_modules_embeddings_buffers_position_ids_
    buf3 = reader.storage('bdb9058e12e0ea3c440225e45485ad56d7d3f5e7', 75890688, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf3, (49408, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_embeddings_modules_token_embedding_parameters_weight_
    buf4 = reader.storage('c87a4e22dcdc1dfa7a727549c790aea39ffeaa1b', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf4, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_weight_
    buf5 = reader.storage('eb7408b52c3fc5df5ac3c50f703d4f94f2487f14', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf5, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm1_parameters_bias_
    buf6 = reader.storage('713814cb4c2197f7484dc73619b3673762978f60', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf6, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
    buf7 = reader.storage('679ae87ef38ccb20a0d4836e5c8b77c6b76d130c', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf7, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_bias_
    buf8 = reader.storage('c696b7deea9fd298d1725de7d58852cf55182ef2', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf8, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
    buf9 = reader.storage('f12b0023052458bdef241fafb2e2590cc9e81310', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf9, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_bias_
    buf10 = reader.storage('cfb320495a88a7e166a9cd939933bfdd330796ce', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf10, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
    buf11 = reader.storage('c6c6b98c0ce8385f626f0472a80895b36b4a5c7c', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf11, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_bias_
    buf12 = reader.storage('64722ed99bd5cd00985f67c14669c3685b03ad71', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf12, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_weight_
    buf13 = reader.storage('70cd286411375bb7f949cb8c5a05188500fa1fd1', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf13, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_self_attn_modules_out_proj_parameters_bias_
    buf14 = reader.storage('a2c6e83216257e74678eb839ad7313274e00e3a7', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf14, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_weight_
    buf15 = reader.storage('af9be36a796ee0a654e93aa2d0baf80ee531b489', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf15, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_layer_norm2_parameters_bias_
    buf16 = reader.storage('952af602e1306713ce5def5a5564142e85cb9865', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf16, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_weight_
    buf17 = reader.storage('0fb7c17f50338e85c6048ba88e479eaa057841d6', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf17, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc1_parameters_bias_
    buf18 = reader.storage('7f764dd51f9347b6841855d41802cbf287280713', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf18, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_weight_
    buf19 = reader.storage('698998b10d8b410d2085b6d941823171e4e37f4b', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf19, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_0_modules_mlp_modules_fc2_parameters_bias_
    buf20 = reader.storage('b72b901a72d72c60019e31a0c6aaa4b3255bad84', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf20, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_weight_
    buf21 = reader.storage('d829ca83004d48a4ed11cb6f85c02d0543e58733', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf21, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm1_parameters_bias_
    buf22 = reader.storage('deb292b3cc60a637c25ba0ae6021d591d2bd81b8', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf22, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_
    buf23 = reader.storage('3ba0c7a0ce92fa351b4389e021de507e2dd9256d', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf23, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_bias_
    buf24 = reader.storage('bc63b09fd03aa7c5b5c265701f6dc61ab4f5fcbd', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf24, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_
    buf25 = reader.storage('ae69c8f558f11a8d7be4f7c8bddaa0cf4e97de8d', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf25, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_bias_
    buf26 = reader.storage('a859de47238c1cbc9ede497822cdbc84795de1a8', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf26, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_
    buf27 = reader.storage('84fe55a22e659db199252ee3f2161e668a145919', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf27, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_bias_
    buf28 = reader.storage('2fb9f14ff15f3a7385a96dffc7afab1e264b9141', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf28, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_weight_
    buf29 = reader.storage('82d8439d9ba3ad7cd37b421593ec46c3592bd4b9', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf29, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_self_attn_modules_out_proj_parameters_bias_
    buf30 = reader.storage('75d1622f50c90ceeb8fcb3409ec2cb67622f0f3f', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf30, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_weight_
    buf31 = reader.storage('0560d81197c454ef846ec39322e6f30bfe76a620', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf31, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_layer_norm2_parameters_bias_
    buf32 = reader.storage('334a6de13e15e0d8e5aebdd4ac2acc9a86fd342e', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf32, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_weight_
    buf33 = reader.storage('73f45cf19b66321b4bd655219154529b6655f92b', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf33, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc1_parameters_bias_
    buf34 = reader.storage('7e9aaa01684877c5432c5afb55b455cb9303e151', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf34, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_weight_
    buf35 = reader.storage('8a2c8643d5129d1f1464f62525d1fc5d7d1a4de4', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf35, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_1_modules_mlp_modules_fc2_parameters_bias_
    buf36 = reader.storage('41aef8f1fb1b26901ac2d35c5e3bb220eb79fdf8', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf36, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_weight_
    buf37 = reader.storage('751f017a636aa414e8a875e0f60c3103873e11eb', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf37, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm1_parameters_bias_
    buf38 = reader.storage('44c7e885d70e2a68637b1bb9de617f0662ac4566', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf38, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_
    buf39 = reader.storage('630b4a1382e9b08a0e9511100b13c3711df35733', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf39, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_bias_
    buf40 = reader.storage('183f1630eb0241b6c23ca98992b26d85d09f8104', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf40, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_
    buf41 = reader.storage('4ba22d942ccce4f359f0d153458fa7941851d2b0', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf41, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_bias_
    buf42 = reader.storage('cdb14d3e47ef193f571d35888f9f56cde4ce427f', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf42, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_
    buf43 = reader.storage('9ac0016b8ee6d393e0ee1712087a1156b64d90dc', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf43, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_bias_
    buf44 = reader.storage('1a4a31d5d4db9378134930544fc343387690123c', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf44, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_weight_
    buf45 = reader.storage('276ee403ed267a9e84d650825ed7454f990deb59', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf45, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_self_attn_modules_out_proj_parameters_bias_
    buf46 = reader.storage('c90f8951ce0350e31e774b52d1a546952bf5621f', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf46, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_weight_
    buf47 = reader.storage('67ce16b8052c9358f283786e3350eded92072b5a', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf47, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_layer_norm2_parameters_bias_
    buf48 = reader.storage('7b0fe4293a94def530d8789fcc56e92ae03c6792', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf48, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_weight_
    buf49 = reader.storage('6a0a5578736f85daf6af4ef1d9a00a3a1a11650d', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf49, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc1_parameters_bias_
    buf50 = reader.storage('47f05b511109df2ebb57bce9c9fff74c626b1b19', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf50, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_weight_
    buf51 = reader.storage('f2a9c72509757e5f0110a4226e8863628ee32891', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf51, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_2_modules_mlp_modules_fc2_parameters_bias_
    buf52 = reader.storage('4cc75ed91e5dd12f17b50b2989d4d2cf99ba56e0', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf52, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_weight_
    buf53 = reader.storage('60f87d2cbaf9b7ef2b32cf3042d88b5eaa6d28c5', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf53, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm1_parameters_bias_
    buf54 = reader.storage('270c27bd3a6acb629e7f775a8ed4bbb27fba7e21', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf54, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_
    buf55 = reader.storage('e45eac67dbe6641000eae5cdd1a0a1ed430101a7', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf55, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_bias_
    buf56 = reader.storage('d6a5e1e4150fbc0e2bcea7b26f17c01ce944732f', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf56, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_
    buf57 = reader.storage('46abefda9a299ec9230e7eefae10c41b641cf58f', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf57, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_bias_
    buf58 = reader.storage('73292b0b9ae36dd5862d862b8b0bcabb9fdba57c', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf58, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_
    buf59 = reader.storage('1aa43ba559f96bc74f0053d6486fe55dc90d0249', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf59, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_bias_
    buf60 = reader.storage('1498aa11d048d5814c31c5522acc1fa3cce37405', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf60, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_weight_
    buf61 = reader.storage('19ab06821984c06c091da2ea8b6cc2fad1a6f828', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf61, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_self_attn_modules_out_proj_parameters_bias_
    buf62 = reader.storage('0711a2debde1ff52b08b3bccc478214482897bb9', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf62, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_weight_
    buf63 = reader.storage('40bb6d61d5405b8c0d41a2079349656c19341fb1', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf63, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_layer_norm2_parameters_bias_
    buf64 = reader.storage('f38f9aa97c83b65326ed2675f0f075ad62935f2e', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf64, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_weight_
    buf65 = reader.storage('69fe1c1053e4e6a28694ee2875d0ab414093e513', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf65, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc1_parameters_bias_
    buf66 = reader.storage('9852c92931f0e09166234c5b2f6e496ff4927494', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf66, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_weight_
    buf67 = reader.storage('c2a873b410fc4cb95781d557e17f7246cc68cfce', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf67, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_3_modules_mlp_modules_fc2_parameters_bias_
    buf68 = reader.storage('0b6350c4fa2a8f3f4ea3cc91aae45e7579507133', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf68, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_weight_
    buf69 = reader.storage('849a7e2e11c7a80dd6fcfe7393928df6025a066b', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf69, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm1_parameters_bias_
    buf70 = reader.storage('f1be02c72b6418e7c11f9ae1057c7560d347c507', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf70, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_
    buf71 = reader.storage('aed6289b4a3d20bf6f8e2c11751b000beee34f83', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf71, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_bias_
    buf72 = reader.storage('81b66f1272745f83803870c9c7b2c389f098dbae', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf72, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_
    buf73 = reader.storage('b2ec8b62b6acb43321f5fc69bffa51da33e4586f', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf73, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_bias_
    buf74 = reader.storage('26ef0005ed527ac6070e408dbc8bfb557b5490ea', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf74, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_
    buf75 = reader.storage('17410888e80c8a0ebab1859f89faec2d512b29e4', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf75, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_bias_
    buf76 = reader.storage('88ce09844f427d25a3b5a841b00c29efefe3fcdf', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf76, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_weight_
    buf77 = reader.storage('31778a98e1fdabde33ab0cec1a3c1589be357ad0', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf77, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_self_attn_modules_out_proj_parameters_bias_
    buf78 = reader.storage('604b4f75e30595155dede0ba9d9ce61248ff85bd', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf78, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_weight_
    buf79 = reader.storage('74ff4c682f4ceee5439e79e0c9413e0a5293530f', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf79, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_layer_norm2_parameters_bias_
    buf80 = reader.storage('9b5f1a57a838c90dc38e1e09c091883d41b52daf', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf80, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_weight_
    buf81 = reader.storage('d0fac6665316211e2af2145a9d1d438f8587f574', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf81, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc1_parameters_bias_
    buf82 = reader.storage('a27fdbe6191191fe411871c9a875361174bea81c', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf82, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_weight_
    buf83 = reader.storage('72a52649f008240f40883ea451d35a01296f1e1a', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf83, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_4_modules_mlp_modules_fc2_parameters_bias_
    buf84 = reader.storage('aefb359d822d131fc90fc6c263d0f08f244b65f3', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf84, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_weight_
    buf85 = reader.storage('ed30a21b88894949bc560200a8d87dc2b6db173b', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf85, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm1_parameters_bias_
    buf86 = reader.storage('03cbb45e7e656f0ddaa13108516489dc2b588aa3', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf86, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_
    buf87 = reader.storage('218efd6c7482510804f9e186527e665518c6e3bd', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf87, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_bias_
    buf88 = reader.storage('9b26e18000b7834cd7275ae031630de7c1bc4e9e', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf88, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_
    buf89 = reader.storage('825267f78b3e081591cf21346c6f8b9787143e07', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf89, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_bias_
    buf90 = reader.storage('2a70b7e9da6a20d522bee06425d5d7cefed0ea95', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf90, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_
    buf91 = reader.storage('070872ced6eb072720e6ca8678b5d0e14bd0db19', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf91, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_bias_
    buf92 = reader.storage('cab449aab1385f6309d3816b83acdd0416f62c8a', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf92, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_weight_
    buf93 = reader.storage('419bb76ca5e816b82a893d090d528c8e3fc9b531', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf93, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_self_attn_modules_out_proj_parameters_bias_
    buf94 = reader.storage('b748896ec4b6159d0dedacb0661d303ae54c390a', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf94, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_weight_
    buf95 = reader.storage('705852f3ae93edb4a8f03cca3c859bcb704e2fc8', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf95, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_layer_norm2_parameters_bias_
    buf96 = reader.storage('c4ae0e685e9a53d9843a8f865e3ef73e075b8b81', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf96, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_weight_
    buf97 = reader.storage('906d0e6fb9f049b1983bd10607a99d1dbcc34355', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf97, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc1_parameters_bias_
    buf98 = reader.storage('d9dde4fdab63b4ecca9f6c8f22cadacc7a8f25e5', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf98, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_weight_
    buf99 = reader.storage('05df2f4300aeb90945d2715b06a74e1e888d2564', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf99, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_5_modules_mlp_modules_fc2_parameters_bias_
    buf100 = reader.storage('36b95655af1f4d371fb6ecc23414d36dd792ca0f', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf100, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_weight_
    buf101 = reader.storage('2fffcfd1f0542a9e73026211c26b55318562ee6c', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf101, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm1_parameters_bias_
    buf102 = reader.storage('72991212c47234d476c52620bf1d649059c9d97d', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf102, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_
    buf103 = reader.storage('4a471f96bde1020c0402f3481961603e8b8c0542', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf103, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_bias_
    buf104 = reader.storage('b4a36996070ddbb31d83dc7a2dfaf30d49e5cf05', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf104, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_
    buf105 = reader.storage('a323e51dd5d58b969b89e6fcb03321079d11dabe', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf105, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_bias_
    buf106 = reader.storage('60722d8eb88cc339af1d460bcda5c2b918d32d39', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf106, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_
    buf107 = reader.storage('7676fa7b41331ef9c9af73228546489733a7e3f5', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf107, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_bias_
    buf108 = reader.storage('de1de090a72123afcee0fa23fe89b9365387e1e5', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf108, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_weight_
    buf109 = reader.storage('b7920118e2fcb7ea3e525d352b14262e37afa296', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf109, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_self_attn_modules_out_proj_parameters_bias_
    buf110 = reader.storage('7255f1fc01447f1c420b1a371a235c5083837de7', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf110, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_weight_
    buf111 = reader.storage('85f1b160f223e55f41c87cee852b1efbb29f51d5', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf111, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_layer_norm2_parameters_bias_
    buf112 = reader.storage('429acee1e44063a1ac7950136026e9c9258582f2', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf112, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_weight_
    buf113 = reader.storage('3d8fa5503822f138743d4bdc7270416c9c1d8bef', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf113, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc1_parameters_bias_
    buf114 = reader.storage('936a0a90f29dec7ede76c008e955e4f9d9bb54cd', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf114, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_weight_
    buf115 = reader.storage('37db185c12fc11fe492dbdc6e2ab65b7ccce72ed', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf115, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_6_modules_mlp_modules_fc2_parameters_bias_
    buf116 = reader.storage('54e7f86f57c85a2781782562ebfe6ed1f517268f', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf116, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_weight_
    buf117 = reader.storage('334f6f3f12410baf3dba21929666264251f40ce2', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf117, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm1_parameters_bias_
    buf118 = reader.storage('51859b38e00ae044445ff47bf7047f11e7711ec4', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf118, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_
    buf119 = reader.storage('7e249a02d8057269c606649d2cacdd94c58e40c0', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf119, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_bias_
    buf120 = reader.storage('f09b7c7b7358100facd019a947a114b8ebe59187', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf120, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_
    buf121 = reader.storage('235351b73c76dec23b14e134b78624974aa94cd1', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf121, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_bias_
    buf122 = reader.storage('9806f1b2b613b73ca94cb3b74c60f57a86ecdc09', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf122, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_
    buf123 = reader.storage('b9bdb68ded55e4ca744b382a830bc547a3706ed1', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf123, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_bias_
    buf124 = reader.storage('1fbace7d762e8a72dd7eeb4e3533a9d98d9733c3', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf124, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_weight_
    buf125 = reader.storage('341e63e32527b45d3a5c25ad05f20405f928927d', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf125, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_self_attn_modules_out_proj_parameters_bias_
    buf126 = reader.storage('77728de6b4dc3b313ff2f31e1eb2a14e1d6fcc98', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf126, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_weight_
    buf127 = reader.storage('ab48a0580a939ce76dba9046e7fd7340c15055ff', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf127, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_layer_norm2_parameters_bias_
    buf128 = reader.storage('714a75158b72f581fe355d005b28c3d0deb4baac', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf128, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_weight_
    buf129 = reader.storage('21260b1c733e1b345825555d592961c7959fea6d', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf129, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc1_parameters_bias_
    buf130 = reader.storage('c93a2de91158f2679da42f79c1783f442a8c28a5', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf130, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_weight_
    buf131 = reader.storage('24aab929d05e487fe534d4e48fc780d1ddc7c33c', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf131, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_7_modules_mlp_modules_fc2_parameters_bias_
    buf132 = reader.storage('146dd14f4a27c2ea56e8da83ee762124786cd16d', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf132, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_weight_
    buf133 = reader.storage('b46e16c9d5e1b91c750ad712659a991d6d4b59d8', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf133, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm1_parameters_bias_
    buf134 = reader.storage('a00b98ceda929085f995c339de55892e7a9437b3', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf134, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_
    buf135 = reader.storage('948e13f72d6ab162673a7e2b2bb64a742d21a242', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf135, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_bias_
    buf136 = reader.storage('85c40d1a58cc7e59f49806214d3ce2b017ce0885', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf136, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_
    buf137 = reader.storage('10c9c06777effaae85b567b2a61940fe3b9be363', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf137, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_bias_
    buf138 = reader.storage('40c43d8468cb3edc32ca955dc1740e28d2e7c2b3', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf138, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_
    buf139 = reader.storage('6be66686e2af75563dcc12b3a195cc7e9c96398c', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf139, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_bias_
    buf140 = reader.storage('c43a532242d99c51b0bf35f20c8b652216d1a0c6', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf140, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_weight_
    buf141 = reader.storage('c4f268d409ce2f306bd1a789d6cf9ef0abd5ce09', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf141, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_self_attn_modules_out_proj_parameters_bias_
    buf142 = reader.storage('d7ce3be83810ae6eef50bd6e07535233f4a54ecf', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf142, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_weight_
    buf143 = reader.storage('a7e18610812a267672dd9fcd02a6b8d0fa0ae6ce', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf143, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_layer_norm2_parameters_bias_
    buf144 = reader.storage('e94481f4813bbafd108cdccd1d315f30bc5fd56d', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf144, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_weight_
    buf145 = reader.storage('23848098493ed7efb0dcb91943cab2254cea6deb', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf145, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc1_parameters_bias_
    buf146 = reader.storage('f60dd3c49a82cfe40938919e3696c794c4537c00', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf146, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_weight_
    buf147 = reader.storage('a8b55e1c47985c85a43cee2b6b6525915f419980', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf147, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_8_modules_mlp_modules_fc2_parameters_bias_
    buf148 = reader.storage('2bb739d112cc542c9d98ed091827b256cc601eb7', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf148, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_weight_
    buf149 = reader.storage('e71fec781ca38cf5dd27e3fb5c9b6f49f8d15f28', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf149, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm1_parameters_bias_
    buf150 = reader.storage('c9ef5a916de6380c5df2c40faed774ef5fb49d0f', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf150, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_
    buf151 = reader.storage('5d3cffcbbce854fa7034660a2249d8ba26372555', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf151, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_bias_
    buf152 = reader.storage('28657e979ecf7c02e6ad8a4f9da11666eb05cd07', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf152, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_
    buf153 = reader.storage('e727fd81dcd28ed797be01c517e54900371f349a', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf153, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_bias_
    buf154 = reader.storage('b93fa5095f52d6445f2cb4ee49998eee405518a7', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf154, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_
    buf155 = reader.storage('f7d52df3fc89fd346a94a4f8b631410754960e9c', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf155, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_bias_
    buf156 = reader.storage('cfc18f7e263ee4a3c13eba8ab079d33c0e117713', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf156, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_weight_
    buf157 = reader.storage('f9a4180e2c040852d9dbf8bf1cc3eb6fefec57f6', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf157, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_self_attn_modules_out_proj_parameters_bias_
    buf158 = reader.storage('842d5383f7881d3fc7a85445262e79b5b20c7efd', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf158, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_weight_
    buf159 = reader.storage('975eb6f3b059218f6f212c3a7209cdc5fe186ee0', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf159, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_layer_norm2_parameters_bias_
    buf160 = reader.storage('7d9ec067012cf023cb7879c61a6ca50f5e196ace', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf160, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_weight_
    buf161 = reader.storage('ffb304d4cbdeae4c5c7f3091fa84eddb01a9aa81', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf161, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc1_parameters_bias_
    buf162 = reader.storage('4b89575a4ee4d683cb0b69db4efcbff90a4dec88', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf162, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_weight_
    buf163 = reader.storage('3da197506408dcb7f821c3b2f67c29c6f6a2a86a', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf163, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_9_modules_mlp_modules_fc2_parameters_bias_
    buf164 = reader.storage('70060f4dcda9e7d6c637fc4165749574bd26b2ed', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf164, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_weight_
    buf165 = reader.storage('7db24b02921619168c9429da367cf83e48963b07', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf165, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm1_parameters_bias_
    buf166 = reader.storage('11ee16c54feeeec151881e149c99f57b56dd150d', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf166, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_
    buf167 = reader.storage('7abc6b614055f94eefacc5f69d93f6bb14d08150', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf167, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_bias_
    buf168 = reader.storage('9ef9b32a727649fc9cde02261c5cfbdebf3a9460', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf168, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_
    buf169 = reader.storage('1bc9b38b6bbd7499d447304d3264194f47ceed44', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf169, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_bias_
    buf170 = reader.storage('1090438132ad84066220d9bb406c3e94718cca3c', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf170, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_
    buf171 = reader.storage('ceb6f7fab171c3bcae6605172cfbf6b7e4bc8159', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf171, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_bias_
    buf172 = reader.storage('d1f48044478b9475167dce7b7ce19448ce28e393', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf172, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_weight_
    buf173 = reader.storage('50d05704c8482264126e84f0dd36ba9bed0c67e2', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf173, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_self_attn_modules_out_proj_parameters_bias_
    buf174 = reader.storage('a0c8d01c32796b790f64942135d35e64b5767173', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf174, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_weight_
    buf175 = reader.storage('34b30fa1ab994ec3d0f7ccca87c97efee1596313', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf175, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_layer_norm2_parameters_bias_
    buf176 = reader.storage('dcd64c6258274c1a6a3ae17de847f63c8ba1f94f', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf176, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_weight_
    buf177 = reader.storage('7bfa3472d247447e862940129884bf823ceefdb2', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf177, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc1_parameters_bias_
    buf178 = reader.storage('818fd5249398d85aca28304ff18080b7b40a64df', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf178, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_weight_
    buf179 = reader.storage('21943171ae0f1f30ea3204959786da02eff68ebf', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf179, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_10_modules_mlp_modules_fc2_parameters_bias_
    buf180 = reader.storage('8f398f76392a5f5a58074b8bb13af174c80149cd', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf180, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_weight_
    buf181 = reader.storage('0c2639809445df4dd4011c3bc039fa6ced0ec77e', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf181, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm1_parameters_bias_
    buf182 = reader.storage('b9b2d2d75b3877f42d7676544db08cecdd0fff4d', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf182, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_
    buf183 = reader.storage('7f084bbc409166ba92faa429e94454fa4773bdde', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf183, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_bias_
    buf184 = reader.storage('1eff690cfb34bfc4e2e1ca8ff33a1d890ee54a03', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf184, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_
    buf185 = reader.storage('837be660281cd6708725f5eb5b2f3a74cd3a292e', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf185, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_bias_
    buf186 = reader.storage('38fd27d98dd0ac6836f9ab97eb0cb9f31dbb9db8', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf186, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_
    buf187 = reader.storage('0006b4d859007283fc42dacd0e4b99218b4b77ad', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf187, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_bias_
    buf188 = reader.storage('a13131110816524d4d1f845e4e9c878793608222', 1179648, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf188, (768, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_weight_
    buf189 = reader.storage('3b0a9a9472160bdf7cf275b39a00339123ac2aa9', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf189, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_self_attn_modules_out_proj_parameters_bias_
    buf190 = reader.storage('a85a8aafc49ef433892001e64f1d0460a3193696', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf190, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_weight_
    buf191 = reader.storage('00eb219e088e1a3a2d6ed80d20699cbfb9122f3e', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf191, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_layer_norm2_parameters_bias_
    buf192 = reader.storage('11038fb916b65ac55a773940c3848b8b3937d4fc', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf192, (3072, 768), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_weight_
    buf193 = reader.storage('f685828e08cdc05f127e0216e065d9f6ae6ea7b5', 6144, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf193, (3072,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc1_parameters_bias_
    buf194 = reader.storage('476ac306c6d609f1d7bbda3e5b3b91c2aab83fd8', 4718592, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf194, (768, 3072), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_weight_
    buf195 = reader.storage('4605c2e0ae91c15cff3cd7e3a4e83cd7192c3034', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf195, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_encoder_modules_layers_modules_11_modules_mlp_modules_fc2_parameters_bias_
    buf196 = reader.storage('0e59805176b17d6490f7540fdbd660c387e1445d', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf196, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_final_layer_norm_parameters_weight_
    buf197 = reader.storage('a7b77d33caa053c4984aee2f832d62a752d620fb', 1536, device=device(type='cuda', index=0), dtype_hint=torch.bfloat16)
    reader.tensor(buf197, (768,), dtype=torch.bfloat16, requires_grad=True, is_leaf=True)  # L_self_modules_text_model_modules_final_layer_norm_parameters_bias_
load_args._version = 0

if __name__ == '__main__':
    from torch._dynamo.repro.after_dynamo import run_repro
    run_repro(mod, load_args, accuracy=False, command='minify',
        save_dir='/workspaces/nikbauer34/tbank_imagegen/InstantCharacter/torch_compile_debug/run_2025_07_13_19_11_47_695074-pid_8562/minifier/checkpoints', autocast=False, backend=None)
